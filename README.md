## Project Overview

This project learns the probability density function (PDF) of a transformed NOâ‚‚ concentration variable using a Generative Adversarial Network (GAN).

The model:
-Does not assume any parametric distribution
-Learns the distribution purely from data samples
-Uses generator samples to approximate the PDF via KDE

## Dataset

Source: India Air Quality Dataset (Kaggle)

Feature Used: NOâ‚‚ concentration

Missing values removed before processing

# Step 1: Variable Transformation

Each NOâ‚‚ value ğ‘¥ is transformed using:


Where:



For Roll Number 102303064:

a_r = 1.5
b_r = 1.5

# Step 2: GAN Architecture

The GAN learns the unknown distribution of the transformed variable ğ‘§.

Generator
Input: 5D Gaussian noise

Layers:
Linear(5 â†’ 32) + ReLU
Linear(32 â†’ 32) + ReLU
Linear(32 â†’ 1)

Discriminator
Input: 1D sample
Layers:
Linear(1 â†’ 32) + ReLU
Linear(32 â†’ 32) + ReLU
Linear(32 â†’ 1) + Sigmoid

Training Details
Loss: Binary Cross Entropy
Optimizer: Adam
Learning Rate: 0.0002
Epochs: 2000
Batch Size: 128
Data normalized for stability

The discriminator distinguishes:
Real samples: 
ğ‘§
Fake samples: 
ğº(ğœ–), where ğœ–âˆ¼ğ‘(0,1)

# Step 3: PDF Estimation

After training:
10,000 samples generated from the trained generator
Kernel Density Estimation (KDE) applied
Estimated PDF plotted and compared with real distribution

# Results
![GAN PDF](pdf.png)

âœ” Mode Coverage
Dominant mode captured successfully
No visible mode collapse

âœ” Training Stability
Balanced generatorâ€“discriminator losses
Stable adversarial training

âœ” Distribution Quality
Right-skew structure learned
Good alignment with real distribution in dense regions
Minor deviation in extreme tails


ğŸš€ How to Run
pip install torch pandas numpy matplotlib scikit-learn
python gan_training.py

ğŸ“ Key Takeaway

This project demonstrates that GANs can learn complex probability distributions purely from samples, without assuming Gaussian or any analytical form.